{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime as dt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import dataframe_image as dfi\n",
    "import pickle # for model persistence\n",
    "\n",
    "from Project1 import (\n",
    "    load_and_process_raw_data, \n",
    "    get_columns_and_types,\n",
    "    generate_choropleth,\n",
    "    estimate_y_from_X_ols,\n",
    "    add_seasonality_features,\n",
    "    day_map,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# CRISP-DM Flow for [Boston Airbnb Dataset](https://www.kaggle.com/datasets/airbnb/boston?resource=download)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "- Customers of Airbnb - hosts and guests - ultimately need to converge on price for a transaction to occur.\n",
    "- So a fundamental question is: what factors influence prices?\n",
    "    - **Price** will be the dependent variable.\n",
    "    - I will explore the dataset to determine our **independent variables** (predictors for a model).\n",
    "- This information can be used in variety of ways. For example, Airbnb could give automatic pricing guidance to new listings based on core criteria, and renters can view similar properties by filtering on that criteria before making a booking decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "Given raw data from Kaggle, what variables in the dataset can be used to predict price?\n",
    "- How clean is it, how much can be used directly, and how much would need to be processed/feature engineered to be usable?\n",
    "- Use visualizations, aggregation, filtering to better understand and prepare the data.\n",
    "- Fill missing data where possible, exclude data where justifiable.\n",
    "\n",
    "I will focus on 3 core questions to get a sense of how certain variables influene price:\n",
    "- How does **geography** influence price?\n",
    "- How do **listing characteristics** influence price?\n",
    "- How does **time/seasonality** influence price?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### First, load and clean the data\n",
    "- I did not ultimately use *monthly_price*/*weekly_price* in the model, but I used OLS to estimate the missing values based on *price*.\n",
    "- I used a KNN classifier to fill in missing zip codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_listings, boston_calendar, boston_reviews = load_and_process_raw_data()\n",
    "\n",
    "# identify column types\n",
    "boston_listing_col_types = get_columns_and_types(boston_listings)\n",
    "boston_calendar_col_types = get_columns_and_types(boston_calendar)\n",
    "boston_review_col_types = get_columns_and_types(boston_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Are there outliers in *price*?\n",
    "- Looking at histograms of price shows us there are at least 2 outliers where prices are $3000 or more. Also, it appears the bulk of the dataset has prices below $1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_cutoff = 500\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 3))\n",
    "boston_listings['price'].plot.hist(\n",
    "    title='Distribution of All Prices', ax=axes[0]);\n",
    "boston_listings['price'].sort_values(ascending=False).head(100).plot.hist(\n",
    "    title='Distribution of Largest 100 Prices', ax=axes[1]);\n",
    "boston_listings['price'].where(lambda x: x <= outlier_cutoff).dropna().plot.hist(\n",
    "    title=f'Distribution of Prices\\nLess Than/Equal to ${outlier_cutoff}', ax=axes[2]);\n",
    "plt.savefig('price_histograms.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### What percentage of observations are below the outlier cutoff (say, below $500)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'Percent of observations more than ${outlier_cutoff}: {\n",
    "    len(boston_listings['price'].where(lambda x: x > outlier_cutoff).dropna())/len(boston_listings):.1%\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Explore other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "#### These are object/string column labels that are not all NaN\n",
    "- A quick look at one of the observations shows several fields that are free text, such as notes/neighborood_overview/description. Processing these is beyond the scope of this project.\n",
    "- However, other fields look more usable, such as neighbnorhood_cleaned/zipcode/room_type/cancellation_policy, etc.\n",
    "- Note: I did some back and forth manual investigating here to hone in variables that looked interesting to me. I am not showing full details of that process as it is repetitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_listings[boston_listing_col_types['object']].dropna(how='all', axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### These are integer column labels that are not all NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_listings[boston_listing_col_types['int']].dropna(how='all', axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### These are float column listings that are not all NaN\n",
    "- A handful of these look like they could be categorical, like bathrooms/bedrooms/beds, even though they are stored as floats.\n",
    "    - For example, a unit could have 1.5 bathrooms. \n",
    "    - I will check the number of unique values for each of these fields, to verfify this.\n",
    "- Review scores are included in a few variations: these should be incldued among our numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_listings[boston_listing_col_types['float']].dropna(how='all', axis=1).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### As suspected, the fields bedrooms/bathrooms/beds each only contain 7-13 unique values. We will consider these among out categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in ['bedrooms', 'bathrooms', 'beds']:\n",
    "    print(f'{label} has {len(boston_listings[label].unique())} unique values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "- Decide on a subset of features to use for a parsimonious model.\n",
    "- We'll define a set of categorical variables and another set of numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_vars_to_use = [\n",
    "    'bathrooms',               # number of bathrooms\n",
    "    'bedrooms',                # number of bedrooms\n",
    "    'beds',                    # number of beds\n",
    "    'accommodates',            # number of occupants that can be accomodated \n",
    "    'property_type',           # House, Apartment, Condoninium, etc\n",
    "    'room_type',               # Entire home/apt, Private room, Shared room\n",
    "    'cancellation_policy',     # moderate, flexible, strict, super_strict_30\n",
    "    'host_identity_verified',  # f, t (boolean)\n",
    "    'neighbourhood_cleansed',  # cleaned neighborhood label\n",
    "]\n",
    "\n",
    "numeric_features_to_use = [\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value',\n",
    "    'zipcode_cleaned'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Now, look through our chosen variables and note the frequency of missing values (as a count and a percentge of all values).\n",
    "While we could explore filling some of these values, I think given the relatively low occurrences means I can leave the missing values in, but use an algorithm than can handle the nans. I'm thinking a decision tree, where missing values aren't ignored (but can ratehr form branches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_nan_details = pd.concat({\n",
    "    'NaN Count': boston_listings[['price'] + categorical_vars_to_use + numeric_features_to_use].isnull().sum(),\n",
    "    'NaN %': boston_listings[['price'] + categorical_vars_to_use + numeric_features_to_use].isnull().sum()/len(boston_listings),\n",
    "}, axis=1).rename_axis('Feature', axis=0).style.format('{:.1%}', subset='NaN %').set_caption('Missing Value Details for Selected Features')\n",
    "display(variable_nan_details)\n",
    "\n",
    "dfi.export(variable_nan_details, 'variable_nan_details.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### **Question 1: How does geography influence Airbnb rental prices?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "##### Add a choropleth (geographical map) and overlay the listings prices from the dataset.\n",
    "- Thank you for the geojson data: https://github.com/codeforgermany/click_that_hood/blob/main/public/data/boston.geojson?short_path=46589b4\n",
    "- Use log scale for the prices (so the colorbar isn't too compressed) - or change the scale of the colorbar.\n",
    "\n",
    "A quick visual inspection of the map allow us to see more expensive listings tend to northward, and in the following neighborhoods (in no particular order):\n",
    "- West End, North End, South End, Downtown, Leather District, Chinatown, Leather District, South Boston Waterfront, Fenway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_choropleth(df=boston_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_by_neighborhood = pd.concat([\n",
    "    boston_listings[['price', 'neighbourhood_cleansed']].groupby('neighbourhood_cleansed').mean().squeeze().to_frame('Mean Price'),\n",
    "    boston_listings[['price', 'neighbourhood_cleansed']].groupby('neighbourhood_cleansed').median().squeeze().to_frame('Median Price'),\n",
    "], axis=1).sort_values(by='Median Price', ascending=False)\n",
    "prices_by_neighborhood_styled = prices_by_neighborhood.style.format('${:.0f}').set_caption('Mean and Median Price by Neighborhood<br>Sorted by Median')\n",
    "dfi.export(prices_by_neighborhood_styled, 'prices_by_neighborhood_table.png')\n",
    "prices_by_neighborhood_styled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "ax = prices_by_neighborhood['Median Price'].plot.bar(cmap='viridis', title='Sorted Bar Plot of Median Price by Neighborhood')\n",
    "ax.axvline(11.5, color='black', alpha=0.2)\n",
    "ax.axhline(150, color='black', alpha=0.2)\n",
    "fig.tight_layout()\n",
    "plt.savefig('prices_by_prices_by_neighborhood_barplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### **Question 2: How do characteristics influence Airbnb listing prices?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = sns.pairplot(\n",
    "    data=boston_listings[['price'] + categorical_vars_to_use + numeric_features_to_use].dropna(),\n",
    ")\n",
    "grid.fig.suptitle('Pairplot of Price, Categorical Variables and Numerica Variables');\n",
    "grid.fig.tight_layout()\n",
    "grid.fig.savefig('variable_pairplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### **Question 3: How does time influence Airbnb listing prices?**\n",
    "- Day of week.\n",
    "- Week of month.\n",
    "- Month of year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_data = boston_calendar.assign(log_price=lambda x: np.log(x['price'])).dropna()\n",
    "ax = sns.boxplot(\n",
    "    data=boxplot_data, x='month', y='log_price', showfliers=False, \n",
    "    palette={month: 'salmon' if month in [9, 10, 11] else 'dodgerblue' for month in boxplot_data.month.unique()},\n",
    "    hue='month', legend=False,\n",
    ")\n",
    "ax.set_title(\"Distribution of Log Listing Prices by Calendar Month\");\n",
    "plt.savefig('price_by_month_boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(\n",
    "    data=boxplot_data, x='weekday', y='log_price', showfliers=False,\n",
    "    palette={weekday: 'salmon' if weekday in [4, 5] else 'dodgerblue' for weekday in boxplot_data.weekday.unique()},\n",
    "    hue='weekday', legend=False,\n",
    ")\n",
    "ax.set_title(\"Distribution of Log Listing Prices by Day of the Week\");\n",
    "ax.set_xticks(ax.get_xticks());\n",
    "ax.set_xticklabels(day_map[i] for i in ax.get_xticks());\n",
    "plt.savefig('price_by_weekday_boxplot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Data Modeling\n",
    "- Select an appropriate algorithm to model price given selected predcitors.\n",
    "- Algo should be robust to missing values, and not senstive to potential correlation among the independent variables.\n",
    "- Algo should be able to utilize both numeric and categorical values among the independent variables.\n",
    "- Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Result Evaluation\n",
    "- Result evaluation: evaluate model fit in train/testing.\n",
    "- Store the trained model so it can be used by a third party."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "- Predict prices using the stored model and testing/out of sample data.\n",
    "- Summarize findings in a post on Medium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Three questions/topics to explore:\n",
    "- How does **geography** inlfuence Airbnb **prices** in Boston?\n",
    "    - What areas/zip codes/neighborhoods are more expensive than others?\n",
    "- How do **property characteristics** influence Airbnb prices in Boston?\n",
    "    - Number of bathrooms, bedrooms, beds, square footage, reviews, etc.\n",
    "- How does **time (seasonality)** influence Airbnb prices in Boston?\n",
    "    - Period of the week (days), period of the month (weeks), period of the year (months)?\n",
    "    - We only have about 2 years of data to work with, but we'll see what comes out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# Let's look at how well a regression can fit price on latitude and longitude\n",
    "- This gives a very poor fit, even on the training set. Why?\n",
    "    - It could be that latitude and longtide - which reflect locations on a sphere (the Earth) do not align well with 2d locations on a map.\n",
    "    - Therefore, the link betweeb price and lat/lon may not be linear, which means regression is not the right tool.\n",
    "    - Let's keep exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_price_on_lat_lon = estimate_y_from_X_ols(\n",
    "    data=boston_listings,\n",
    "    y_label='price',\n",
    "    X_labels=['latitude', 'longitude'],\n",
    "    train_size=0.6,\n",
    "    random_state=42,\n",
    "    add_constant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "What other numerical values can we use (and what needs to be filled before we can)?\n",
    "- host_response_rate (missing around 400 entries)\n",
    "- host_acceptance_rate (missing around 400 entries)\n",
    "- square_feet (missing a lot)\n",
    "- extra people?\n",
    "- cleaning fee?\n",
    "- security_deposit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "# Assemble a model\n",
    "- Us a random forest regressor. This can handle both categorical and numeric values, and also can handle nans (especially in the numeric values, like the reviews, which contain numerous nans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = 'price'\n",
    "outlier_ids = list(boston_listings['price'].where(lambda x: x > outlier_cutoff).dropna().index)\n",
    "\n",
    "numeric_features_to_use = [\n",
    "    'review_scores_accuracy',\n",
    "    'review_scores_cleanliness',\n",
    "    'review_scores_checkin',\n",
    "    'review_scores_communication',\n",
    "    'review_scores_location',\n",
    "    'review_scores_value',\n",
    "    'zipcode_cleaned',\n",
    "]\n",
    "\n",
    "categorical_vars_to_use = [\n",
    "    'bathrooms',               # number of bathrooms\n",
    "    'bedrooms',                # number of bedrooms\n",
    "    'beds',                    # number of beds\n",
    "    'accommodates',            # number of occupants that can be accomodated \n",
    "    'property_type',           # House, Apartment, Condoninium, etc\n",
    "    'room_type',               # Entire home/apt, Private room, Shared room\n",
    "    'cancellation_policy',     # moderate, flexible, strict, super_strict_30\n",
    "    'host_identity_verified',  # f, t (boolean)\n",
    "    'neighbourhood_cleansed',  # cleaned neighborhood label\n",
    "]\n",
    "\n",
    "model_data = pd.concat([\n",
    "    boston_listings[[y_label]],\n",
    "    pd.get_dummies(boston_listings[categorical_vars_to_use], columns=categorical_vars_to_use, drop_first=True),\n",
    "    boston_listings[numeric_features_to_use],\n",
    "    ], axis=1).drop(outlier_ids)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    model_data.drop(y_label, axis=1), \n",
    "    model_data[y_label],\n",
    "    train_size=0.6, \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# train model\n",
    "m_rf = RandomForestRegressor(\n",
    "    random_state=42,\n",
    ")\n",
    "m_rf.fit(X_train, y_train);\n",
    "\n",
    "# store model\n",
    "with open('model_random_forest.pkl', 'wb') as f:\n",
    "    pickle.dump(m_rf, f, protocol=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "\n",
    "scatter_data_train = pd.concat({\n",
    "        'Y_HAT': pd.Series(m_rf.predict(X_train)),\n",
    "        'Y_OBS': y_train.reset_index(drop=True),\n",
    "    }, axis=1)\n",
    "scatter_data_test = pd.concat({\n",
    "        'Y_HAT': pd.Series(m_rf.predict(X_test)),\n",
    "        'Y_OBS': y_test.reset_index(drop=True),\n",
    "    }, axis=1)\n",
    "\n",
    "sns.regplot(\n",
    "    data=scatter_data_train,\n",
    "    y='Y_HAT',\n",
    "    x='Y_OBS',\n",
    "    ax=axes[0],\n",
    "    ci=None,\n",
    "    line_kws={'color': 'dodgerblue'},\n",
    ")\n",
    "axes[0].set_title(f'Random Forest\\nTraining Data, Score: {m_rf.score(X_train, y_train):.2f}')\n",
    "\n",
    "sns.regplot(\n",
    "    data=scatter_data_test,\n",
    "    y='Y_HAT',\n",
    "    x='Y_OBS',\n",
    "    ax=axes[1],\n",
    "    ci=None,\n",
    "    line_kws={'color': 'dodgerblue'},\n",
    ")\n",
    "axes[1].set_title(f'Random Forest\\nTesting Data: Score: {m_rf.score(X_test, y_test):.2f}')\n",
    "fig.tight_layout()\n",
    "\n",
    "ymin = pd.concat([scatter_data_test, scatter_data_train], axis=0).min().min()\n",
    "ymax = pd.concat([scatter_data_test, scatter_data_train], axis=0).max().max()\n",
    "for i in axes:\n",
    "    ax.set_ylim((ymin, ymax))\n",
    "\n",
    "plt.savefig('model_train_test.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
